Word2Vec essentially is a shallow 2-layer neural network trained.
The input contains all the documents/texts in our training set. For the network to process these texts, they are represented in a 1-hot encoding of the words.
The number of neurons present in the hidden layer is equal to the length of the embedding we want. That is, if we want all our words to be vectors of length 300, then the hidden layer will contain 300 neurons.
Press enter or click to view image in full size
Understanding the neural network training of Word2Vec model
The output layer contains probabilities for a target word (given an input to the model, what word is expected) given a particular input.
At the end of the training process, the hidden weights are treated as the word embedding. Intuitively, this can be thought of as each word having a set of n weights (300 considering the example above) “weighing” their different characteristics (an analogy we used earlier).